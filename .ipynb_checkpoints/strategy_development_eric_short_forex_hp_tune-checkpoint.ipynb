{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ib_insync import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels import regression,stats\n",
    "import math\n",
    "import datetime \n",
    "import statsmodels.formula.api as smf \n",
    "from datetime import date, time, datetime, timedelta\n",
    "from collections import deque\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter \n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(country, adr, fx_dict):\n",
    "    adr_path = f'eric_jh_data/{country}/{adr}/adr.csv'\n",
    "    stock_path =  f'eric_jh_data/{country}/{adr}/underlying.csv'\n",
    "    fx_path = fx_dict[country][0]\n",
    "    fx_type =  fx_dict[country][1]\n",
    "\n",
    "    adr_df = pd.read_csv(adr_path, index_col = 0).rename(columns = {'close':'adr_close', 'open':'adr_open'})\n",
    "    stock_df = pd.read_csv(stock_path, index_col = 0).rename(columns = {'close':'stock_close', 'open':'stock_open'})\n",
    "    fx_df = pd.read_csv(fx_path, index_col = 0)\n",
    "\n",
    "    merged_df = pd.merge(adr_df.loc[:,['date', 'adr_open','adr_close']], stock_df.loc[:,['date', 'stock_open','stock_close']])\n",
    "    merged_df = pd.merge(merged_df, fx_df)\n",
    "\n",
    "    if fx_type == 1:\n",
    "        merged_df['stock_open_usd'] = merged_df['stock_open']/((merged_df['avg_bid_non_us_at'] + merged_df['avg_ask_non_us_at'])/2)\n",
    "    else:\n",
    "        merged_df['stock_open_usd'] = merged_df['stock_open']*((merged_df['avg_bid_non_us_at'] + merged_df['avg_ask_non_us_at'])/2)\n",
    "    merged_df[\"ratio\"] = merged_df['stock_open_usd']/merged_df['adr_close']\n",
    "    \n",
    "    ratio_geq_1 = True\n",
    "    if np.mean(merged_df[\"ratio\"] < 1):\n",
    "        merged_df[\"ratio\"] = 1/merged_df[\"ratio\"]\n",
    "        ratio_geq_1 = False\n",
    "    \n",
    "    return ratio_geq_1, np.round(np.mean(merged_df[\"ratio\"]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'eric_jh_data/'\n",
    "countries = sorted(['Australia', 'Japan', 'China'])\n",
    "fx_dict = {'Australia':('eric_jh_data/Forex/AUD_USD_new.csv',0),\n",
    "           'Japan':('eric_jh_data/Forex/USD_JPY_new.csv',1),\n",
    "           'China':('eric_jh_data/Forex/USD_HKD_new.csv',1)}\n",
    "\n",
    "list_pairs = []\n",
    "for country in countries:\n",
    "    countrypath = mypath + country\n",
    "    adr_names =  [f for f in listdir(countrypath) if not isfile(join(countrypath, f))] #grab all adr names of the country\n",
    "    for adr in sorted(adr_names):\n",
    "        list_pairs.append((country, adr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Australia, ADR_Stock: ATHE_ATH, Estimated Ratio (4 d.p.): 59.4889, Implied Ratio (2 s.f.): 60.0\n",
      "Country: Australia, ADR_Stock: GENE_GTG, Estimated Ratio (4 d.p.): 595.5978, Implied Ratio (2 s.f.): 600.0\n",
      "Country: Australia, ADR_Stock: IMMP_IMM, Estimated Ratio (4 d.p.): 9.9082, Implied Ratio (2 s.f.): 10.0\n",
      "Country: Australia, ADR_Stock: IMRN_IMC, Estimated Ratio (4 d.p.): 39.4289, Implied Ratio (2 s.f.): 40.0\n",
      "Country: Australia, ADR_Stock: JHX_JHX, Estimated Ratio (4 d.p.): 1.0043, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Australia, ADR_Stock: KZIA_KZA, Estimated Ratio (4 d.p.): 10.0346, Implied Ratio (2 s.f.): 10.0\n",
      "Country: Australia, ADR_Stock: MESO_MSB, Estimated Ratio (4 d.p.): 5.0155, Implied Ratio (2 s.f.): 5.0\n",
      "Country: Australia, ADR_Stock: PLL_PLL, Estimated Ratio (4 d.p.): 101.5788, Implied Ratio (2 s.f.): 100.0\n",
      "Country: Australia, ADR_Stock: WBK_WBC, Estimated Ratio (4 d.p.): 0.9997, Implied Ratio (2 s.f.): 1.0\n",
      "Country: China, ADR_Stock: ACH_2600, Estimated Ratio (4 d.p.): 24.9288, Implied Ratio (2 s.f.): 25.0\n",
      "Country: China, ADR_Stock: BGNE_6160, Estimated Ratio (4 d.p.): 12.9902, Implied Ratio (2 s.f.): 13.0\n",
      "Country: China, ADR_Stock: CEA_670, Estimated Ratio (4 d.p.): 49.9009, Implied Ratio (2 s.f.): 50.0\n",
      "Country: China, ADR_Stock: HNP_902, Estimated Ratio (4 d.p.): 39.8606, Implied Ratio (2 s.f.): 40.0\n",
      "Country: China, ADR_Stock: LFC_2628, Estimated Ratio (4 d.p.): 4.9939, Implied Ratio (2 s.f.): 5.0\n",
      "Country: China, ADR_Stock: PTR_857, Estimated Ratio (4 d.p.): 99.8525, Implied Ratio (2 s.f.): 100.0\n",
      "Country: China, ADR_Stock: SHI_338, Estimated Ratio (4 d.p.): 99.9224, Implied Ratio (2 s.f.): 100.0\n",
      "Country: China, ADR_Stock: SNP_386, Estimated Ratio (4 d.p.): 99.9663, Implied Ratio (2 s.f.): 100.0\n",
      "Country: China, ADR_Stock: ZNH_1055, Estimated Ratio (4 d.p.): 49.8338, Implied Ratio (2 s.f.): 50.0\n",
      "Country: Japan, ADR_Stock: CAJ_7751, Estimated Ratio (4 d.p.): 0.9995, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Japan, ADR_Stock: HMC_7267, Estimated Ratio (4 d.p.): 0.9999, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Japan, ADR_Stock: IX_8591, Estimated Ratio (4 d.p.): 5.0055, Implied Ratio (2 s.f.): 5.0\n",
      "Country: Japan, ADR_Stock: MFG_8411, Estimated Ratio (4 d.p.): 4.9999, Implied Ratio (2 s.f.): 5.0\n",
      "Country: Japan, ADR_Stock: MUFG_8306, Estimated Ratio (4 d.p.): 0.9995, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Japan, ADR_Stock: NMR_8604, Estimated Ratio (4 d.p.): 0.9998, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Japan, ADR_Stock: SMFG_8316, Estimated Ratio (4 d.p.): 5.0032, Implied Ratio (2 s.f.): 5.0\n",
      "Country: Japan, ADR_Stock: SONY_6758, Estimated Ratio (4 d.p.): 1.0009, Implied Ratio (2 s.f.): 1.0\n",
      "Country: Japan, ADR_Stock: TAK_4502, Estimated Ratio (4 d.p.): 2.001, Implied Ratio (2 s.f.): 2.0\n",
      "Country: Japan, ADR_Stock: TM_7203, Estimated Ratio (4 d.p.): 2.0001, Implied Ratio (2 s.f.): 2.0\n"
     ]
    }
   ],
   "source": [
    "# Store ratios\n",
    "for (country, adr) in list_pairs:\n",
    "    ratio_geq_1, ratio = get_ratio(country, adr, fx_dict)\n",
    "    if adr == \"ACH_2600\" or adr == \"BGNE_6160\":\n",
    "        rounded_ratio = float('%.2g' % ratio)\n",
    "    else:\n",
    "        rounded_ratio = float('%.1g' % ratio)\n",
    "    print(\"Country: {}, ADR_Stock: {}, Estimated Ratio (4 d.p.): {}, Implied Ratio (2 s.f.): {}\".format(country, adr, ratio, rounded_ratio))\n",
    "    ratio_df = pd.DataFrame({\"ratio_geq_1\" : [ratio_geq_1], \"ratio\" : [rounded_ratio]})\n",
    "    ratio_df.to_csv(f'eric_jh_data/{country}/{adr}/ratio.csv')\n",
    "        \n",
    "# This shows the empircally estimated ratio, and the implied ratio we shall assume.\n",
    "# These values corroborate with the select few we checked online, like GENE_GTG and BGNE_6160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(country, adr, fx_dict, forex_bps = 10, adjust_forex_expense = True):\n",
    "    adr_path = f'eric_jh_data/{country}/{adr}/adr.csv'\n",
    "    stock_path =  f'eric_jh_data/{country}/{adr}/underlying.csv'\n",
    "    ratio_path = f'eric_jh_data/{country}/{adr}/ratio.csv'\n",
    "    fx_path = fx_dict[country][0]\n",
    "    fx_type =  fx_dict[country][1]\n",
    "\n",
    "    adr_df = pd.read_csv(adr_path, index_col = 0).rename(columns = {'close':'adr_close', 'open':'adr_open', 'volume' : 'adr_volume'})\n",
    "    stock_df = pd.read_csv(stock_path, index_col = 0).rename(columns = {'close':'stock_close', 'open':'stock_open', 'volume' : 'stock_volume'})\n",
    "    fx_df = pd.read_csv(fx_path, index_col = 0)\n",
    "    ratio_df = pd.read_csv(ratio_path, index_col = 0)\n",
    "\n",
    "    # Invert fx data so that all prices are reflected in USD\n",
    "    if fx_type == 0:\n",
    "        inverted_fx_df = 1/fx_df.iloc[:,[2,1,3,5,4,6,8,7,9,11,10,12]].copy()\n",
    "        inverted_fx_df.columns = fx_df.columns[1:-1]\n",
    "        fx_df.iloc[:,1:-1] = inverted_fx_df\n",
    "    merged_df = pd.merge(adr_df.loc[:,['date', 'adr_open','adr_close', 'adr_volume']], stock_df.loc[:,['date', 'stock_open','stock_close', 'stock_volume']])\n",
    "    merged_df = pd.merge(merged_df, fx_df)\n",
    "    ratio_geq_1, ratio = ratio_df[\"ratio_geq_1\"].item(), ratio_df[\"ratio\"].item()\n",
    "\n",
    "#     ratio is (stock price in USD)/(ADR price)\n",
    "#     If ratio >= 1, we shall buy 1 stock, and sell multiple adrs\n",
    "#     If ratio < 1, we shall sell 1 adr, and buy multiple stocks\n",
    "    if ratio_geq_1:\n",
    "        merged_df[\"stock_num_per_unit\"] = 1\n",
    "        merged_df[\"adr_num_per_unit\"] = ratio\n",
    "        merged_df[\"stock_open_per_unit\"] = merged_df[\"stock_open\"]\n",
    "        merged_df[\"stock_close_per_unit\"] = merged_df[\"stock_close\"]\n",
    "        merged_df[\"adr_open_per_unit\"] = merged_df[\"adr_open\"]*ratio\n",
    "        merged_df[\"adr_close_per_unit\"] = merged_df[\"adr_close\"]*ratio\n",
    "    else:\n",
    "        merged_df[\"stock_num_per_unit\"] = ratio\n",
    "        merged_df[\"adr_num_per_unit\"] = 1\n",
    "        merged_df[\"stock_open_per_unit\"] = merged_df[\"stock_open\"]*ratio\n",
    "        merged_df[\"stock_close_per_unit\"] = merged_df[\"stock_close\"]*ratio\n",
    "        merged_df[\"adr_open_per_unit\"] = merged_df[\"adr_open\"]\n",
    "        merged_df[\"adr_close_per_unit\"] = merged_df[\"adr_close\"]    \n",
    "    \n",
    "    if adjust_forex_expense:\n",
    "        # Added expense for trading small amounts in forex market\n",
    "        forex_bid_multiplier = 1 - 0.0001*forex_bps\n",
    "        forex_ask_multiplier = 1 + 0.0001*forex_bps\n",
    "        merged_df.loc[:,merged_df.columns.str.contains(\"bid\")] *= forex_bid_multiplier\n",
    "        merged_df.loc[:,merged_df.columns.str.contains(\"ask\")] *= forex_ask_multiplier\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adr_open</th>\n",
       "      <th>adr_close</th>\n",
       "      <th>adr_volume</th>\n",
       "      <th>stock_open</th>\n",
       "      <th>stock_close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>avg_bid_non_us_before</th>\n",
       "      <th>avg_ask_non_us_before</th>\n",
       "      <th>avg_non_us_before</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_bid_us_at</th>\n",
       "      <th>avg_ask_us_at</th>\n",
       "      <th>avg_us_at</th>\n",
       "      <th>ir</th>\n",
       "      <th>stock_num_per_unit</th>\n",
       "      <th>adr_num_per_unit</th>\n",
       "      <th>stock_open_per_unit</th>\n",
       "      <th>stock_close_per_unit</th>\n",
       "      <th>adr_open_per_unit</th>\n",
       "      <th>adr_close_per_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>18.08</td>\n",
       "      <td>17.36</td>\n",
       "      <td>1873</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>987003</td>\n",
       "      <td>1.296948</td>\n",
       "      <td>1.299612</td>\n",
       "      <td>1.298280</td>\n",
       "      <td>...</td>\n",
       "      <td>1.302409</td>\n",
       "      <td>1.305068</td>\n",
       "      <td>1.303738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.4</td>\n",
       "      <td>18.08</td>\n",
       "      <td>17.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-13</td>\n",
       "      <td>17.76</td>\n",
       "      <td>16.16</td>\n",
       "      <td>2762</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1586945</td>\n",
       "      <td>1.303310</td>\n",
       "      <td>1.305987</td>\n",
       "      <td>1.304648</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315530</td>\n",
       "      <td>1.318215</td>\n",
       "      <td>1.316872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>17.76</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-14</td>\n",
       "      <td>16.96</td>\n",
       "      <td>17.08</td>\n",
       "      <td>2545</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2905099</td>\n",
       "      <td>1.316900</td>\n",
       "      <td>1.319606</td>\n",
       "      <td>1.318253</td>\n",
       "      <td>...</td>\n",
       "      <td>1.309820</td>\n",
       "      <td>1.312529</td>\n",
       "      <td>1.311174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>16.96</td>\n",
       "      <td>17.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>17.68</td>\n",
       "      <td>17.52</td>\n",
       "      <td>2106</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2069419</td>\n",
       "      <td>1.312108</td>\n",
       "      <td>1.314822</td>\n",
       "      <td>1.313465</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310301</td>\n",
       "      <td>1.312994</td>\n",
       "      <td>1.311647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>22.8</td>\n",
       "      <td>17.68</td>\n",
       "      <td>17.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>17.40</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2378678</td>\n",
       "      <td>1.301459</td>\n",
       "      <td>1.304133</td>\n",
       "      <td>1.302796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282035</td>\n",
       "      <td>1.284634</td>\n",
       "      <td>1.283335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>21.6</td>\n",
       "      <td>17.40</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  adr_open  adr_close  adr_volume  stock_open  stock_close  \\\n",
       "0  2015-04-10     18.08      17.36        1873       0.040        0.039   \n",
       "1  2015-04-13     17.76      16.16        2762       0.039        0.039   \n",
       "2  2015-04-14     16.96      17.08        2545       0.036        0.038   \n",
       "3  2015-04-15     17.68      17.52        2106       0.039        0.038   \n",
       "4  2015-04-16     17.40      17.00        1068       0.038        0.036   \n",
       "\n",
       "   stock_volume  avg_bid_non_us_before  avg_ask_non_us_before  \\\n",
       "0        987003               1.296948               1.299612   \n",
       "1       1586945               1.303310               1.305987   \n",
       "2       2905099               1.316900               1.319606   \n",
       "3       2069419               1.312108               1.314822   \n",
       "4       2378678               1.301459               1.304133   \n",
       "\n",
       "   avg_non_us_before  ...  avg_bid_us_at  avg_ask_us_at  avg_us_at  ir  \\\n",
       "0           1.298280  ...       1.302409       1.305068   1.303738 NaN   \n",
       "1           1.304648  ...       1.315530       1.318215   1.316872 NaN   \n",
       "2           1.318253  ...       1.309820       1.312529   1.311174 NaN   \n",
       "3           1.313465  ...       1.310301       1.312994   1.311647 NaN   \n",
       "4           1.302796  ...       1.282035       1.284634   1.283335 NaN   \n",
       "\n",
       "   stock_num_per_unit  adr_num_per_unit  stock_open_per_unit  \\\n",
       "0               600.0                 1                 24.0   \n",
       "1               600.0                 1                 23.4   \n",
       "2               600.0                 1                 21.6   \n",
       "3               600.0                 1                 23.4   \n",
       "4               600.0                 1                 22.8   \n",
       "\n",
       "   stock_close_per_unit  adr_open_per_unit  adr_close_per_unit  \n",
       "0                  23.4              18.08               17.36  \n",
       "1                  23.4              17.76               16.16  \n",
       "2                  22.8              16.96               17.08  \n",
       "3                  22.8              17.68               17.52  \n",
       "4                  21.6              17.40               17.00  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stock_num_per_unit is how many stocks we would buy for 1 \"unit\" of trade\n",
    "# avg_bid_non_us_before is how much foreign currency we can buy with 1 USD, 1 minute before the Asian market opens\n",
    "# avg_bid_non_us_at is how much foreign currency we can buy with 1 USD, when the Asian market opens\n",
    "# avg_bid_us_before is how much foreign currency we can buy with 1 USD, 1 minute before the US market opens\n",
    "# avg_bid_us_at is how much foreign currency we can buy with 1 USD, when the US market opens\n",
    "# All dates are in local time: so in sequential order (for each row), it will go stock_open, stock_close, adr_open, adr_close\n",
    "merged_df = data_processing(*list_pairs[1], fx_dict)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_drawdown(portfolio_values, method = \"percentage\"):\n",
    "    peak, trough = portfolio_values[0], portfolio_values[0]\n",
    "    max_drawdown = 0\n",
    "    for i in range(1, len(portfolio_values)):\n",
    "        if portfolio_values[i] < trough:\n",
    "            trough = portfolio_values[i]\n",
    "            if method == \"percentage\":\n",
    "                max_drawdown = max(max_drawdown, (peak - trough)/peak)\n",
    "            else:\n",
    "                max_drawdown = max(max_drawdown, peak - trough)\n",
    "        elif portfolio_values[i] > peak:\n",
    "            peak, trough = portfolio_values[i], portfolio_values[i]\n",
    "    return max_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_statistics(stock_values, adr_values, var_ci):\n",
    "    port_stock = stock_values - adr_values\n",
    "    port = pd.DataFrame(data = port_stock)\n",
    "    port_diff = port - port.shift(1)\n",
    "    pnl = pd.DataFrame(port_diff).dropna()\n",
    "    sigma = pnl.std()[0]\n",
    "    pnl['pct_rank'] = pnl.rank(pct=True)\n",
    "    pnl.columns =['daily_pl', 'pct_rank']\n",
    "    var = abs(pnl[pnl.pct_rank< 1-var_ci].daily_pl.max())\n",
    "    max_drawdown_abs = calc_max_drawdown(port_stock, \"absolute\")\n",
    "    return sigma, var, max_drawdown_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(date, cash, country, adr, num_xticks = 15, save = False, filename = \"pnl_plot.png\"):\n",
    "    fig = plt.figure(figsize = (20, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(date, cash)\n",
    "    plt.xticks(np.arange(0, len(date), (len(date) - 1 )// num_xticks), rotation = 30, ha = 'right', fontsize = 14)\n",
    "    plt.xlim(0, len(date))\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.grid(True)\n",
    "    plt.title(f'PnL Chart for {adr} pair from {country}', fontsize = 18)\n",
    "    if save:\n",
    "        fig.savefig(f'eric_jh_data/{country}/{adr}/{filename}')\n",
    "    else:\n",
    "        plt.show();\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variant 1 - Begin each trade on Asian market open (Evaluate after US market closes)\n",
    "\n",
    "To open a position, we check the CLOSE price of adr, compared it to CLOSE px of \n",
    "stock of the same row. We buy the stock on the next trading OPEN for Asian/US market\n",
    "\n",
    "To close a position,  we check the CLOSE price of adr, compared it to CLOSE px of \n",
    "stock of the same row. We sell the stock on the next trading next OPEN for Asian/US market\n",
    "\n",
    "For each row:\n",
    "    stock_open, stock_close, adr_open, adr_close\n",
    "    After these 4 events, assess condition (right before the Asian market opens ~ 6.59PM EST)\n",
    "    Place trade on next row (First trade stock on Asian market open, then trade ADR on US market open)\n",
    "    \n",
    "start_date: First date (EST) we may place a trade\n",
    "end_date: Last date (EST) we may place a trade\n",
    "portfolio_values: Stores value of portfolio at each date from one day before the start_date, to the end_date (inclusive), when Asian market opens (EST ~ 7:00 PM)\n",
    "\"\"\"\n",
    "def pairs_trade_v1(merged_df, lookback = 100, cash = 250000, entry = 1, exit = 0, stop_loss = 3, \n",
    "                   start_date = \"2016-01-01\", end_date = \"2021-01-31\", slippage_bps = 10, \n",
    "                   borrowing_bps = 50, risk_lookback = 100, var_ci = 0.95, var_limit = 0.1, max_drawdown_limit = 0.2, \n",
    "                   sigma_limit = 0.05, maximum_holding_period = 30, volume_lookback = 5):\n",
    "    \n",
    "    # Accounts for slippage and transaction costs\n",
    "    short_multiplier = 1 - 0.0001*slippage_bps\n",
    "    long_multiplier = 1 + 0.0001*slippage_bps\n",
    "    starting_cash = cash\n",
    "    stock_pos, adr_pos = 0, 0\n",
    "    # For book-keeping, since we shall store the portfolio value of the day before\n",
    "    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "    forex_cash = 0\n",
    "    holding_period = None\n",
    "    diff_record = deque(maxlen = lookback)\n",
    "    trade_records = []\n",
    "    portfolio_values = []\n",
    "    dates = []\n",
    "    hits = []\n",
    "    \n",
    "    # Make sure that merged_df before end date is not empty\n",
    "    if merged_df[merged_df['date'] < end_date].empty:\n",
    "        return 0, trade_records, portfolio_values, hits, dates\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "\n",
    "        if index+1 < len(merged_df) and index > 0:\n",
    "            \n",
    "            # Add portfolio value for the day before\n",
    "            prev_date = merged_df.loc[index - 1, \"date\"]\n",
    "            if row[\"date\"] >= start_date and prev_date <= end_date:\n",
    "                dates.append(prev_date)\n",
    "                prev_forex_value = forex_cash + stock_pos*row[\"stock_open\"]\n",
    "                if prev_forex_value > 0:\n",
    "                    prev_forex_value /= row['avg_ask_non_us_at']\n",
    "                else:\n",
    "                    prev_forex_value /= row['avg_bid_non_us_at']\n",
    "                \n",
    "                portfolio_values.append(prev_cash + prev_adr_pos*merged_df.loc[index - 1, 'adr_close'] \n",
    "                                        + prev_forex_value)\n",
    "\n",
    "            diff_record.append(row['adr_close_per_unit'] \n",
    "                                   - row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before'])\n",
    "\n",
    "            # We place one trade the day itself (Asian), one trade the day after (US)\n",
    "            if len(diff_record) < lookback or row[\"date\"] < start_date or merged_df.loc[index+1, \"date\"] > end_date:\n",
    "                continue\n",
    "            \n",
    "            # Update cash/adr position after portfolio values has been updated\n",
    "            if stock_pos > 0:\n",
    "                holding_period += 1\n",
    "                cash -= 0.0001*borrowing_bps*(1/252)*abs(adr_pos)*merged_df.loc[index - 1, 'adr_close']\n",
    "                multiplier = (1 + 0.01*(2 + row[\"ir\"])*(1/252))\n",
    "                forex_cash *= multiplier\n",
    "            prev_cash, prev_adr_pos = cash, adr_pos\n",
    "\n",
    "            mean = np.array(diff_record).mean()\n",
    "            std = np.array(diff_record).std()\n",
    "            \n",
    "            # If we have passed the initial lookback window and are in the specified dates\n",
    "            # enter the position if diff is significant\n",
    "            if diff_record[-1] > mean + entry*std and diff_record[-1] <= mean + stop_loss*std:\n",
    "                if stock_pos == 0 and adr_pos == 0:\n",
    "                    portfolio_value_before_entering = cash\n",
    "                    adr_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                    stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                    units = int(min(cash/row['adr_close_per_unit'],\n",
    "                                    cash/(row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before']), \n",
    "                                    adr_volume, \n",
    "                                    stock_volume))\n",
    "                    adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                    stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                    \n",
    "                    # Take portfolio value for each previous day (till today) right before the Asian market opens\n",
    "                    temp_risk_lookback = min(risk_lookback, index)\n",
    "                    current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                    next_day = merged_df.loc[(index - temp_risk_lookback + 2):(index + 1)].copy()\n",
    "                    stock_values = (np.array((current[\"stock_close\"])/np.array(next_day[\"avg_non_us_before\"]))*stock_quantity) \n",
    "                    adr_values = np.array(current[\"adr_close\"]*adr_quantity)\n",
    "                    sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                    if (var > portfolio_value_before_entering*var_limit or \n",
    "                        max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                        sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                        frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                   (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                  (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                        units = int(frac*units)\n",
    "                        if units == 0:\n",
    "                            continue\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"])                        \n",
    "                    \n",
    "                    stock_pos += stock_quantity\n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                    forex_cash -= stock_px_fx*stock_quantity\n",
    "                    # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                    \n",
    "                    adr_pos -= adr_quantity\n",
    "                    adr_px = merged_df.loc[index+1,'adr_open']*short_multiplier\n",
    "                    cash += adr_quantity*adr_px\n",
    "                    \n",
    "                    holding_period = 0\n",
    "                    trade_records.append(\"Opening positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "\n",
    "            # Liquidation condition\n",
    "            elif (diff_record[-1] < mean + exit*std or \n",
    "                  diff_record[-1] > mean + stop_loss*std or \n",
    "                  holding_period == maximum_holding_period):\n",
    "                if stock_pos > 0 and adr_pos < 0 : \n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                    forex_cash += stock_px_fx*stock_pos\n",
    "                    if forex_cash > 0:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                    else:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                    cash += forex_cash\n",
    "                    forex_cash = 0\n",
    "                    \n",
    "                    # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                    \n",
    "                    adr_px = merged_df.loc[index+1,'adr_open']*long_multiplier\n",
    "                    cash -= abs(adr_pos)*adr_px\n",
    "                    trade_records.append(\"Closing positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "                    stock_pos, adr_pos = 0, 0\n",
    "                    holding_period = None\n",
    "                    if cash > portfolio_value_before_entering:\n",
    "                        hits.append(1)\n",
    "                    else:\n",
    "                        hits.append(0)\n",
    "\n",
    "    ret = (portfolio_values[-1] - starting_cash)/starting_cash\n",
    "    \n",
    "    return ret, trade_records, portfolio_values, hits, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variant 2 - Begin each trade on US market open (Evaluate after Asian market closes)\n",
    "\n",
    "To open a position, we check the CLOSE price of adr of the previous row, compared to CLOSE px of \n",
    "stock of the current row. We buy the stock on the next trading OPEN for Asian/US market\n",
    "\n",
    "To close a position, we check the CLOSE price of adr of the previous row, compared to CLOSE px of \n",
    "stock of the current row. We sell the stock on the next trading next OPEN for Asian/US market\n",
    "\n",
    "For each row:\n",
    "    stock_open, stock_close, (assess), adr_open, adr_close\n",
    "    After first 2 events events, assess condition (right before the US market opens ~ 9.29AM EST)\n",
    "    Place trade on current and next row (First trade ADR on US market open, then trade stock on Asian market open)\n",
    "    \n",
    "start_date: First date (EST) we may place a trade\n",
    "end_date: Last date (EST) we may place a trade\n",
    "portfolio_values: Stores value of portfolio at each date from one day before the start_date, to the end_date, when the Asian market opens\n",
    "\"\"\"\n",
    "def pairs_trade_v2(merged_df, lookback = 100, cash = 250000, entry = 1, exit = 0, stop_loss = 3, \n",
    "                   start_date = \"2016-01-01\", end_date = \"2021-01-31\", slippage_bps = 10, \n",
    "                   borrowing_bps = 50, risk_lookback = 100, var_ci = 0.95, var_limit = 0.1, max_drawdown_limit = 0.2, \n",
    "                   sigma_limit = 0.05, maximum_holding_period = 30, volume_lookback = 5):\n",
    "    \n",
    "    # Accounts for slippage and transaction costs\n",
    "    short_multiplier = 1 - 0.0001*slippage_bps\n",
    "    long_multiplier = 1 + 0.0001*slippage_bps\n",
    "    starting_cash = cash\n",
    "    stock_pos, adr_pos = 0, 0\n",
    "    holding_period = None\n",
    "    forex_cash = 0\n",
    "    diff_record = deque(maxlen = lookback)\n",
    "    trade_records = []\n",
    "    portfolio_values = []\n",
    "    dates = []\n",
    "    hits = []\n",
    "    \n",
    "    # Make sure that merged_df before end date is not empty\n",
    "    if merged_df[merged_df['date'] < end_date].empty:\n",
    "        return 0, trade_records, portfolio_values, hits, dates\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        \n",
    "        if index+1 < len(merged_df) and index > 0:\n",
    "            \n",
    "            # Add portfolio value for the day before\n",
    "            prev_date = merged_df.loc[index - 1, \"date\"]\n",
    "            if row[\"date\"] >= start_date and prev_date <= end_date:\n",
    "                dates.append(prev_date)\n",
    "                prev_forex_value = forex_cash + stock_pos*row[\"stock_open\"]\n",
    "                if prev_forex_value > 0:\n",
    "                    prev_forex_value /= row['avg_ask_non_us_at']\n",
    "                else:\n",
    "                    prev_forex_value /= row['avg_bid_non_us_at']\n",
    "                \n",
    "                portfolio_values.append(cash + adr_pos*merged_df.loc[index - 1, 'adr_close'] \n",
    "                                        + prev_forex_value)\n",
    "\n",
    "            diff_record.append(merged_df.loc[index-1,'adr_close_per_unit'] \n",
    "                                   - row['stock_close_per_unit']/row['avg_us_before'])\n",
    "            \n",
    "            # We place both trades the day itself\n",
    "            if len(diff_record) < lookback or row[\"date\"] < start_date or row[\"date\"] > end_date:\n",
    "                continue\n",
    "\n",
    "            if stock_pos > 0:\n",
    "                holding_period += 1\n",
    "                cash -= 0.0001*borrowing_bps*(1/252)*abs(adr_pos)*merged_df.loc[index - 1, 'adr_close']\n",
    "                multiplier = (1 + 0.01*(2 + row[\"ir\"])*(1/252))\n",
    "                forex_cash *= multiplier\n",
    "                \n",
    "            mean = np.array(diff_record).mean()\n",
    "            std = np.array(diff_record).std()\n",
    "            \n",
    "            # If we have passed the initial lookback window and are in the specified dates\n",
    "            # enter the position if diff is significant\n",
    "            if diff_record[-1] > mean + entry*std and diff_record[-1] <= mean + stop_loss*std:\n",
    "                if stock_pos == 0 and adr_pos == 0:\n",
    "                    portfolio_value_before_entering = cash\n",
    "                    adr_volume = 0.2*(merged_df.loc[index-volume_lookback:index - 1,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                    stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                    units = int(min(cash/merged_df.loc[index-1,'adr_close_per_unit'],\n",
    "                                    cash/(row['stock_close_per_unit']/row['avg_us_before']), \n",
    "                                    adr_volume, \n",
    "                                    stock_volume))\n",
    "                    adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                    stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                    # Take portfolio value for each previous day (till today) right before the US market opens\n",
    "                    temp_risk_lookback = min(risk_lookback, index)\n",
    "                    current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                    stock_values = np.array((current[\"stock_close\"]/current[\"avg_us_before\"])*stock_quantity) \n",
    "                    adr_values = np.array(merged_df.loc[(index - temp_risk_lookback):(index-1)][\"adr_close\"]*adr_quantity)\n",
    "                    sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                    if (var > portfolio_value_before_entering*var_limit or \n",
    "                        max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                        sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                        frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                   (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                  (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                        units = int(frac*units)\n",
    "                        if units == 0:\n",
    "                            continue\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"])  \n",
    "                    \n",
    "                    adr_pos -= adr_quantity\n",
    "                    adr_px = row['adr_open']*short_multiplier\n",
    "                    cash += adr_quantity*adr_px\n",
    "                    \n",
    "                    stock_pos += stock_quantity\n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                    forex_cash -= stock_px_fx*stock_quantity\n",
    "#                     stock_px = stock_px_fx/merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "#                     cash -= stock_px*stock_quantity\n",
    "                    \n",
    "                    holding_period = 0\n",
    "                    trade_records.append(\"Opening positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "\n",
    "            # Liquidation condition\n",
    "            elif (diff_record[-1] < mean + exit*std or \n",
    "                  diff_record[-1] > mean + stop_loss*std or \n",
    "                  holding_period == maximum_holding_period):\n",
    "                if stock_pos > 0 and adr_pos < 0 : \n",
    "                    adr_px = row['adr_open']*long_multiplier\n",
    "                    cash -= abs(adr_pos)*adr_px\n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                    forex_cash += stock_px_fx*stock_pos\n",
    "                    if forex_cash > 0:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                    else:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                    cash += forex_cash\n",
    "                    forex_cash = 0\n",
    "                    \n",
    "                    trade_records.append(\"Closing positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                    stock_pos, adr_pos = 0, 0\n",
    "                    holding_period = None\n",
    "                    if cash > portfolio_value_before_entering:\n",
    "                        hits.append(1)\n",
    "                    else:\n",
    "                        hits.append(0)\n",
    "\n",
    "    ret = (portfolio_values[-1] - starting_cash)/starting_cash\n",
    "    \n",
    "    return ret, trade_records, portfolio_values, hits, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variant 3a \n",
    "- Begin each trade on either US market open or Asian market open\n",
    "- Regressions are done for the similar \"type\" of trade\n",
    "    i.e. if we are entering at a certain time, we do a regression based on the values obtained at the same time each day\n",
    "\n",
    "For each row:\n",
    "    stock_open, stock_close, (assess condition 1), adr_open, adr_close, (assess condition 2)\n",
    "    If not condition 2 - No action taken: \n",
    "        After first 2 events, Assess condition 1 (right before the US market opens ~ 9.29AM EST)\n",
    "        If condition 1:\n",
    "            Place trade on current and next row (First trade ADR on US market open, then trade stock on Asian market open)\n",
    "    If not condition 1 - No action taken:\n",
    "        After next 2 events occur, assess condition 2\n",
    "        If condition 2:\n",
    "            Place trade on next row (First trade ADR on Asian market open, then trade stock on US market open)\n",
    "    \n",
    "start_date: First date (EST) we may place a trade\n",
    "end_date: Last date (EST) we may place a trade\n",
    "portfolio_values: Stores value of portfolio at each date from one day before the start_date, to the end_date, when the Asian market opens\n",
    "\"\"\"\n",
    "def pairs_trade_v3a(merged_df, lookback = 100, cash = 250000, entry_cond1_val = 1, entry_cond2_val = 1, \n",
    "                    exit_cond1_val = 0, exit_cond2_val = 0, stop_loss_cond1 = 3, stop_loss_cond2 = 3, \n",
    "                    start_date = \"2016-01-01\", end_date = \"2021-01-31\", slippage_bps = 10, \n",
    "                    borrowing_bps = 50, risk_lookback = 100, var_ci = 0.95, var_limit = 0.1, max_drawdown_limit = 0.2, \n",
    "                    sigma_limit = 0.05, maximum_holding_period = 30, volume_lookback = 5):\n",
    "\n",
    "    # Accounts for slippage and transaction costs\n",
    "    short_multiplier = 1 - 0.0001*slippage_bps\n",
    "    long_multiplier = 1 + 0.0001*slippage_bps\n",
    "    starting_cash = cash\n",
    "    stock_pos, adr_pos = 0, 0\n",
    "    holding_period = None\n",
    "    trade_type = None\n",
    "    forex_cash = 0\n",
    "    # For book-keeping, since we shall store the portfolio value of the day before\n",
    "    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "    diff_record_cond1 = deque(maxlen = lookback)\n",
    "    diff_record_cond2 = deque(maxlen = lookback)\n",
    "    trade_records = []\n",
    "    portfolio_values = []\n",
    "    dates = []\n",
    "    hits = []\n",
    "    enter_cond1, exit_cond1, enter_cond2, exit_cond2 = False, False, False, False\n",
    "    \n",
    "    # Make sure that merged_df before end date is not empty\n",
    "    if merged_df[merged_df['date'] < end_date].empty:\n",
    "        return 0, trade_records, portfolio_values, hits, dates\n",
    "    \n",
    "    for index, row in merged_df.iterrows():\n",
    "                    \n",
    "        if index+1 < len(merged_df) and index > 0:\n",
    "            \n",
    "            # Add portfolio value for the day before\n",
    "            prev_date = merged_df.loc[index - 1, \"date\"]\n",
    "            if row[\"date\"] >= start_date and prev_date <= end_date:\n",
    "                dates.append(prev_date)\n",
    "                prev_forex_value = forex_cash + stock_pos*row[\"stock_open\"]\n",
    "                if prev_forex_value > 0:\n",
    "                    prev_forex_value /= row['avg_ask_non_us_at']\n",
    "                else:\n",
    "                    prev_forex_value /= row['avg_bid_non_us_at']\n",
    "                portfolio_values.append(prev_cash + prev_adr_pos*merged_df.loc[index - 1, 'adr_close'] \n",
    "                                        + prev_forex_value)\n",
    "            \n",
    "            # Before US Market Opens\n",
    "            diff_record_cond1.append(merged_df.loc[index-1,'adr_close_per_unit'] - row['stock_close_per_unit']/row['avg_us_before'])\n",
    "            # Before Asian Market Opens\n",
    "            diff_record_cond2.append(row['adr_close_per_unit'] \n",
    "                                   - row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before'])\n",
    "\n",
    "\n",
    "            # row[\"date\"] is between start_date (inclusive) and end_date (inclusive)\n",
    "            if len(diff_record_cond1) < lookback or row[\"date\"] < start_date or row[\"date\"] > end_date:\n",
    "                continue\n",
    "\n",
    "            # Update cash/adr position after portfolio values has been updated\n",
    "            if stock_pos > 0:\n",
    "                holding_period += 1\n",
    "                cash -= 0.0001*borrowing_bps*(1/252)*abs(adr_pos)*merged_df.loc[index - 1, 'adr_close']\n",
    "                multiplier = (1 + 0.01*(2 + row[\"ir\"])*(1/252))\n",
    "                forex_cash *= multiplier\n",
    "            prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                \n",
    "            mean_cond1 = np.array(diff_record_cond1).mean()\n",
    "            std_cond1 = np.array(diff_record_cond1).std()\n",
    "            mean_cond2 = np.array(diff_record_cond2).mean()\n",
    "            std_cond2 = np.array(diff_record_cond2).std()\n",
    "            \n",
    "            # If a concurrent trade is not already being placed\n",
    "            if not (enter_cond2 or exit_cond2):\n",
    "                enter_cond1 = (diff_record_cond1[-1] > mean_cond1 + entry_cond1_val*std_cond1 \n",
    "                               and diff_record_cond1[-1] <= mean_cond1 + stop_loss_cond1*std_cond1\n",
    "                               and stock_pos == 0 and adr_pos == 0)\n",
    "                exit_cond1 = ((diff_record_cond1[-1] < mean_cond1 + exit_cond1_val*std_cond1 \n",
    "                              or diff_record_cond1[-1] > mean_cond1 + stop_loss_cond1*std_cond1\n",
    "                              or (holding_period == maximum_holding_period and trade_type == 1))\n",
    "                              and stock_pos > 0 and adr_pos < 0)\n",
    "                    \n",
    "                if enter_cond1:\n",
    "                    portfolio_value_before_entering = cash\n",
    "                    adr_volume = 0.2*(merged_df.loc[index-volume_lookback:index - 1,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                    stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                    units = int(min(cash/row['adr_close_per_unit'],\n",
    "                                    cash/(row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before']), \n",
    "                                    adr_volume, \n",
    "                                    stock_volume))\n",
    "                    adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                    stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                    \n",
    "                    # Take portfolio value for each previous day when the Asian market opens\n",
    "                    temp_risk_lookback = min(risk_lookback, index)\n",
    "                    current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                    stock_values = np.array((current[\"stock_close\"]/current[\"avg_us_before\"])*stock_quantity) \n",
    "                    adr_values = np.array(merged_df.loc[(index - temp_risk_lookback):(index-1)][\"adr_close\"]*adr_quantity)\n",
    "                    sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                    if (var > portfolio_value_before_entering*var_limit or \n",
    "                        max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                        sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                        frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                   (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                  (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                        units = int(frac*units)\n",
    "                        if units == 0:\n",
    "                            enter_cond1 = False\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                    if units != 0:\n",
    "                                \n",
    "                        adr_pos -= adr_quantity\n",
    "                        adr_px = row['adr_open']*short_multiplier\n",
    "                        cash += adr_quantity*adr_px\n",
    "\n",
    "                        stock_pos += stock_quantity\n",
    "                        stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                        forex_cash -= stock_px_fx*stock_quantity\n",
    "                        prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                        holding_period = 0\n",
    "                        trade_type = 1\n",
    "                        \n",
    "                        trade_records.append(\"Opening positions:\\n\")\n",
    "                        # Times in EST\n",
    "                        trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                        trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "\n",
    "                elif exit_cond1:\n",
    "                    \n",
    "                    adr_px = row['adr_open']*long_multiplier\n",
    "                    cash -= abs(adr_pos)*adr_px\n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                    forex_cash += stock_px_fx*stock_pos\n",
    "                    if forex_cash > 0:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                    else:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                    cash += forex_cash\n",
    "                    forex_cash = 0\n",
    "                    trade_records.append(\"Closing positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                    stock_pos, adr_pos = 0, 0\n",
    "                    holding_period = None\n",
    "                    trade_type = None\n",
    "                    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                    if cash > portfolio_value_before_entering:\n",
    "                        hits.append(1)\n",
    "                    else:\n",
    "                        hits.append(0)\n",
    "                    \n",
    "            # If a concurrent trade is not already being placed\n",
    "            # The 2nd trade of condition 2 falls on the next day\n",
    "            if not (enter_cond1 or exit_cond1) and merged_df.loc[index+1, \"date\"] <= end_date:\n",
    "                # Check and possibly trade condition 2\n",
    "                enter_cond2 = (diff_record_cond2[-1] > mean_cond2 + entry_cond2_val*std_cond2 \n",
    "                               and diff_record_cond2[-1] <= mean_cond2 + stop_loss_cond2*std_cond2\n",
    "                               and stock_pos == 0 and adr_pos == 0)\n",
    "                exit_cond2 = ((diff_record_cond2[-1] < mean_cond2 + exit_cond2_val*std_cond2 \n",
    "                              or diff_record_cond2[-1] > mean_cond2 + stop_loss_cond2*std_cond2\n",
    "                              or (holding_period == maximum_holding_period and trade_type == 2))\n",
    "                              and stock_pos > 0 and adr_pos < 0)\n",
    "                    \n",
    "                if enter_cond2:\n",
    "                    portfolio_value_before_entering = cash\n",
    "                    adr_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                    stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                    units = int(min(cash/merged_df.loc[index-1,'adr_close_per_unit'],\n",
    "                                    cash/(row['stock_close_per_unit']/row['avg_us_before']), \n",
    "                                    adr_volume, \n",
    "                                    stock_volume))\n",
    "                    adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                    stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                    \n",
    "                    # Take portfolio value for each previous day when the Asian market opens\n",
    "                    temp_risk_lookback = min(risk_lookback, index)\n",
    "                    current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                    next_day = merged_df.loc[(index - temp_risk_lookback + 2):(index + 1)].copy()\n",
    "                    stock_values = (np.array((current[\"stock_close\"])/np.array(next_day[\"avg_non_us_before\"]))*stock_quantity) \n",
    "                    adr_values = np.array(current[\"adr_close\"]*adr_quantity)\n",
    "                    sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                    if (var > portfolio_value_before_entering*var_limit or \n",
    "                        max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                        sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                        frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                   (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                  (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                        units = int(frac*units)\n",
    "                        if units == 0:\n",
    "                            enter_cond2 = False\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"]) \n",
    "                    if units != 0:\n",
    "                        stock_pos += stock_quantity\n",
    "                        stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                        forex_cash -= stock_px_fx*stock_quantity\n",
    "                        # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                        prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                        \n",
    "                        adr_pos -= adr_quantity\n",
    "                        adr_px = merged_df.loc[index+1,'adr_open']*short_multiplier\n",
    "                        cash += adr_quantity*adr_px\n",
    "                        \n",
    "                        holding_period = 0\n",
    "                        trade_type = 2\n",
    "                        trade_records.append(\"Opening positions:\\n\")\n",
    "                        # Times in EST\n",
    "                        trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                        trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "\n",
    "                elif exit_cond2:\n",
    "                    stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                    forex_cash += stock_px_fx*stock_pos\n",
    "                    if forex_cash > 0:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                    else:\n",
    "                        forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                    cash += forex_cash\n",
    "                    forex_cash = 0\n",
    "                    # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                    \n",
    "                    adr_px = merged_df.loc[index+1,'adr_open']*long_multiplier\n",
    "                    cash -= abs(adr_pos)*adr_px\n",
    "                    trade_records.append(\"Closing positions:\\n\")\n",
    "                    # Times in EST\n",
    "                    trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                    trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "                    stock_pos, adr_pos = 0, 0\n",
    "                    holding_period = None\n",
    "                    trade_type = None\n",
    "                    if cash > portfolio_value_before_entering:\n",
    "                        hits.append(1)\n",
    "                    else:\n",
    "                        hits.append(0)\n",
    "\n",
    "    ret = (portfolio_values[-1] - starting_cash)/starting_cash\n",
    "\n",
    "    return ret, trade_records, portfolio_values, hits, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variant 3b\n",
    "- Begin each trade on either US market open or Asian market open\n",
    "- Regressions are done for all data collected in lookback window\n",
    "\n",
    "For each row:\n",
    "    stock_open, stock_close, (assess condition 1), adr_open, adr_close, (assess condition 2)\n",
    "    If not condition 2 - No action taken: \n",
    "        After first 2 events, Assess condition 1 (right before the US market opens ~ 9.29AM EST)\n",
    "        If condition 1:\n",
    "            Place trade on current and next row (First trade ADR on US market open, then trade stock on Asian market open)\n",
    "    If not condition 1 - No action taken:\n",
    "        After next 2 events occur, assess condition 2\n",
    "        If condition 2:\n",
    "            Place trade on next row (First trade ADR on Asian market open, then trade stock on US market open)\n",
    "    \n",
    "start_date: First date (EST) we may place a trade\n",
    "end_date: Last date (EST) we may place a trade\n",
    "portfolio_values: Stores value of portfolio at each date from one day before the start_date, to the end_date, when the Asian market opens\n",
    "\"\"\"\n",
    "def pairs_trade_v3b(merged_df, lookback = 100, cash = 250000, entry_cond1_val = 1, entry_cond2_val = 1, \n",
    "                    exit_cond1_val = 0, exit_cond2_val = 0, stop_loss_cond1 = 3, stop_loss_cond2 = 3, \n",
    "                    start_date = \"2016-01-01\", end_date = \"2021-01-31\", slippage_bps = 10, \n",
    "                    borrowing_bps = 50, risk_lookback = 100, var_ci = 0.95, var_limit = 0.1, max_drawdown_limit = 0.2,\n",
    "                    sigma_limit = 0.05, maximum_holding_period = 30, volume_lookback = 5):\n",
    "    \n",
    "    # Accounts for slippage and transaction costs\n",
    "    short_multiplier = 1 - 0.0001*slippage_bps\n",
    "    long_multiplier = 1 + 0.0001*slippage_bps\n",
    "    # We assume lookback is given in terms of days\n",
    "    lookback *= 2\n",
    "    starting_cash = cash\n",
    "    stock_pos, adr_pos = 0, 0\n",
    "    holding_period = None\n",
    "    trade_type = None\n",
    "    forex_cash = 0\n",
    "    # For book-keeping, since we shall store the portfolio value of the day before\n",
    "    prev_cash, prev_adr_pos = cash, adr_pos\n",
    "    diff_record = deque(maxlen = lookback)\n",
    "    trade_records = []\n",
    "    portfolio_values = []\n",
    "    dates = []\n",
    "    hits = []\n",
    "    enter_cond1, exit_cond1, enter_cond2, exit_cond2 = False, False, False, False\n",
    "    \n",
    "    # Make sure that merged_df before end date is not empty\n",
    "    if merged_df[merged_df['date'] < end_date].empty:\n",
    "        return 0, trade_records, portfolio_values, hits, dates\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "\n",
    "        if index+1 < len(merged_df) and index > 0:\n",
    "            \n",
    "            # Add portfolio value for the day before\n",
    "            prev_date = merged_df.loc[index - 1, \"date\"]\n",
    "            if row[\"date\"] >= start_date and prev_date <= end_date:\n",
    "                dates.append(prev_date)\n",
    "                prev_forex_value = forex_cash + stock_pos*row[\"stock_open\"]\n",
    "                if prev_forex_value > 0:\n",
    "                    prev_forex_value /= row['avg_ask_non_us_at']\n",
    "                else:\n",
    "                    prev_forex_value /= row['avg_bid_non_us_at']\n",
    "                portfolio_values.append(prev_cash + prev_adr_pos*merged_df.loc[index - 1, 'adr_close'] \n",
    "                                        + prev_forex_value)\n",
    "                \n",
    "            # Before US Market Opens\n",
    "            diff_record.append(merged_df.loc[index-1,'adr_close_per_unit'] - row['stock_close_per_unit']/row['avg_us_before'])\n",
    "            if len(diff_record) == lookback and row[\"date\"] >= start_date and row[\"date\"] <= end_date:\n",
    "                if stock_pos > 0:\n",
    "                    holding_period += 1\n",
    "                    cash -= 0.0001*borrowing_bps*(1/252)*abs(adr_pos)*merged_df.loc[index - 1, 'adr_close']\n",
    "                    multiplier = (1 + 0.01*(2 + row[\"ir\"])*(1/252))\n",
    "                    forex_cash *= multiplier\n",
    "                prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                    \n",
    "                mean = np.array(diff_record).mean()\n",
    "                std = np.array(diff_record).std()\n",
    "                \n",
    "                # If a concurrent trade is not already being placed\n",
    "                if not (enter_cond2 or exit_cond2):\n",
    "                    enter_cond1 = (diff_record[-1] > mean + entry_cond1_val*std\n",
    "                                   and diff_record[-1] <= mean + stop_loss_cond1*std\n",
    "                                   and stock_pos == 0 and adr_pos == 0)\n",
    "                    exit_cond1 = ((diff_record[-1] < mean + exit_cond1_val*std\n",
    "                                  or diff_record[-1] > mean + stop_loss_cond1*std\n",
    "                                  or (holding_period == maximum_holding_period and trade_type == 1))\n",
    "                                  and stock_pos > 0 and adr_pos < 0)\n",
    "                    \n",
    "                    if enter_cond1:\n",
    "                        portfolio_value_before_entering = cash\n",
    "                        adr_volume = 0.2*(merged_df.loc[index-volume_lookback:index - 1,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                        stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                        units = int(min(cash/row['adr_close_per_unit'],\n",
    "                                        cash/(row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before']), \n",
    "                                        adr_volume, \n",
    "                                        stock_volume))\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                        \n",
    "                        temp_risk_lookback = min(risk_lookback, index)\n",
    "                        current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                        stock_values = np.array((current[\"stock_close\"]/current[\"avg_us_before\"])*stock_quantity) \n",
    "                        adr_values = np.array(merged_df.loc[(index - temp_risk_lookback):(index-1)][\"adr_close\"]*adr_quantity)\n",
    "                        sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                        if (var > portfolio_value_before_entering*var_limit or \n",
    "                            max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                            sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                            frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                       (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                      (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                            units = int(frac*units)\n",
    "                            if units == 0:\n",
    "                                enter_cond1 = False\n",
    "                            adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                            stock_quantity = int(units*row[\"stock_num_per_unit\"]) \n",
    "                        if units != 0:\n",
    "                            adr_pos -= adr_quantity\n",
    "                            adr_px = row['adr_open']*short_multiplier\n",
    "                            cash += adr_quantity*adr_px\n",
    "\n",
    "                            stock_pos += stock_quantity\n",
    "                            stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                            forex_cash -= stock_px_fx*stock_quantity\n",
    "                            prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                            holding_period = 0\n",
    "                            trade_type = 1\n",
    "                            trade_records.append(\"Opening positions:\\n\")\n",
    "                            # Times in EST\n",
    "                            trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                            trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "\n",
    "                    elif exit_cond1:\n",
    "\n",
    "                        adr_px = row['adr_open']*long_multiplier\n",
    "                        cash -= abs(adr_pos)*adr_px\n",
    "                        stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                        forex_cash += stock_px_fx*stock_pos\n",
    "                        if forex_cash > 0:\n",
    "                            forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                        else:\n",
    "                            forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                        cash += forex_cash\n",
    "                        forex_cash = 0\n",
    "                        trade_records.append(\"Closing positions:\\n\")\n",
    "                        # Times in EST\n",
    "                        trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {row['date']}\\n\")\n",
    "                        trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                        stock_pos, adr_pos = 0, 0\n",
    "                        holding_period = None\n",
    "                        trade_type = None\n",
    "                        prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                        if cash > portfolio_value_before_entering:\n",
    "                            hits.append(1)\n",
    "                        else:\n",
    "                            hits.append(0)\n",
    "                        \n",
    "            # Before Asian Market Opens\n",
    "            diff_record.append(row['adr_close_per_unit'] - row['stock_close_per_unit']/merged_df.loc[index+1,'avg_non_us_before'])\n",
    "            # The 2nd trade of condition 2 falls on the next day\n",
    "            if len(diff_record) == lookback and row[\"date\"] >= start_date and merged_df.loc[index+1,\"date\"] <= end_date:\n",
    "                mean = np.array(diff_record).mean()\n",
    "                std = np.array(diff_record).std()\n",
    "                # If a concurrent trade is not already being placed\n",
    "                if not (enter_cond1 or exit_cond1):\n",
    "                    enter_cond2 = (diff_record[-1] > mean + entry_cond2_val*std\n",
    "                                   and diff_record[-1] <= mean + stop_loss_cond2*std\n",
    "                                   and stock_pos == 0 and adr_pos == 0)\n",
    "                    exit_cond2 = ((diff_record[-1] < mean + exit_cond2_val*std\n",
    "                                  or diff_record[-1] > mean + stop_loss_cond2*std\n",
    "                                  or (holding_period == maximum_holding_period and trade_type == 2))\n",
    "                                  and stock_pos > 0 and adr_pos < 0)\n",
    "\n",
    "                    if enter_cond2:\n",
    "                        portfolio_value_before_entering = cash\n",
    "                        adr_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"adr_volume\"].median()/row[\"adr_num_per_unit\"])\n",
    "                        stock_volume = 0.2*(merged_df.loc[index-volume_lookback+1:index,:][\"stock_volume\"].median()/row[\"stock_num_per_unit\"])\n",
    "                        units = int(min(cash/merged_df.loc[index-1,'adr_close_per_unit'],\n",
    "                                        cash/(row['stock_close_per_unit']/row['avg_us_before']), \n",
    "                                        adr_volume, \n",
    "                                        stock_volume))\n",
    "                        adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                        stock_quantity = int(units*row[\"stock_num_per_unit\"])\n",
    "                        temp_risk_lookback = min(risk_lookback, index)\n",
    "                        current = merged_df.loc[(index - temp_risk_lookback + 1):index].copy()\n",
    "                        next_day = merged_df.loc[(index - temp_risk_lookback + 2):(index + 1)].copy()\n",
    "                        stock_values = (np.array((current[\"stock_close\"])/np.array(next_day[\"avg_non_us_before\"]))*stock_quantity) \n",
    "                        adr_values = np.array(current[\"adr_close\"]*adr_quantity)\n",
    "                        sigma, var, max_drawdown_abs = get_risk_statistics(stock_values, adr_values, var_ci)\n",
    "                        if (var > portfolio_value_before_entering*var_limit or \n",
    "                            max_drawdown_abs > max_drawdown_limit*starting_cash or \n",
    "                            sigma > portfolio_value_before_entering*sigma_limit):\n",
    "                            frac = min((portfolio_value_before_entering*var_limit)/var, \n",
    "                                       (max_drawdown_limit*starting_cash)/max_drawdown_abs,\n",
    "                                      (portfolio_value_before_entering*sigma_limit)/sigma)\n",
    "                            units = int(frac*units)\n",
    "                            if units == 0:\n",
    "                                enter_cond2 = False\n",
    "                            adr_quantity = int(units*row[\"adr_num_per_unit\"])\n",
    "                            stock_quantity = int(units*row[\"stock_num_per_unit\"])  \n",
    "                        if units != 0:\n",
    "                            stock_pos += stock_quantity\n",
    "                            stock_px_fx = merged_df.loc[index+1,'stock_open']*long_multiplier\n",
    "                            forex_cash -= stock_px_fx*stock_quantity\n",
    "                            # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                            prev_cash, prev_adr_pos = cash, adr_pos\n",
    "\n",
    "                            adr_pos -= adr_quantity\n",
    "                            adr_px = merged_df.loc[index+1,'adr_open']*short_multiplier\n",
    "                            cash += adr_quantity*adr_px\n",
    "                            holding_period = 0\n",
    "                            trade_type = 2\n",
    "                            trade_records.append(\"Opening positions:\\n\")\n",
    "                            # Times in EST\n",
    "                            trade_records.append(f\"We bought {stock_quantity} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                            trade_records.append(f\"We sold {adr_quantity} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "\n",
    "                    elif exit_cond2:\n",
    "                        stock_px_fx = merged_df.loc[index+1,'stock_open']*short_multiplier\n",
    "                        forex_cash += stock_px_fx*stock_pos\n",
    "                        if forex_cash > 0:\n",
    "                            forex_cash /= merged_df.loc[index+1,'avg_ask_non_us_at']\n",
    "                        else:\n",
    "                            forex_cash /= merged_df.loc[index+1,'avg_bid_non_us_at']\n",
    "                        cash += forex_cash\n",
    "                        forex_cash = 0\n",
    "                        # We store the current cash/adr position, because the trade below will occur on the next day (EST)\n",
    "                        prev_cash, prev_adr_pos = cash, adr_pos\n",
    "                        \n",
    "                        adr_px = merged_df.loc[index+1,'adr_open']*long_multiplier\n",
    "                        cash -= abs(adr_pos)*adr_px\n",
    "                        trade_records.append(\"Closing positions:\\n\")\n",
    "                        # Times in EST\n",
    "                        trade_records.append(f\"We sold {stock_pos} shares of underlying stock at the price of {stock_px_fx} foreign dollars on {row['date']}\\n\")\n",
    "                        trade_records.append(f\"We bought {-adr_pos} shares of ADR at the price of {adr_px} on {merged_df.loc[index+1,'date']}\\n\")\n",
    "                        stock_pos, adr_pos = 0, 0\n",
    "                        holding_period = None\n",
    "                        trade_type = None\n",
    "                        if cash > portfolio_value_before_entering:\n",
    "                            hits.append(1)\n",
    "                        else:\n",
    "                            hits.append(0)\n",
    "\n",
    "    ret = (portfolio_values[-1] - starting_cash)/starting_cash\n",
    "\n",
    "    return ret, trade_records, portfolio_values, hits, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters:\n",
    "1. Lookback window\n",
    "2. Entry threshold\n",
    "3. Exit threshold\n",
    "4. Stop-loss threshold\n",
    "\n",
    "Steps:\n",
    "1. HP Tune each strategy for each of the pairs\n",
    "2. Store results for each pair in hp_log_sfx{version}.txt\n",
    "3. Store results for each strategy in results_sfx{version}.txt\n",
    "4. Store best strategy for each pair in results_sfx_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_grid = [30, 60, 100]\n",
    "entry_grid = [1, 1.5, 2]\n",
    "exit_grid = [-0.5, 0, 0.5]\n",
    "stop_loss_grid = [2.5, 3, 3.5]\n",
    "oos_start_date = \"2017-04-11\"\n",
    "oos_end_date = \"2018-04-11\"\n",
    "\n",
    "\"\"\"\n",
    "HP Tune on coarse grid\n",
    "\"\"\"\n",
    "\n",
    "def hp_tune(pairs_trade_strategy, version, country, adr, window_grid = [30,60,100], \n",
    "            entry_grid = [1,1.5,2], exit_grid = [-0.5,0,0.5], stop_loss_grid = [2.5,3,3.5], fine = False):\n",
    "    hps = list(itertools.product(*[window_grid, entry_grid, exit_grid, stop_loss_grid]))\n",
    "    merged_df = data_processing(country, adr, fx_dict)\n",
    "    hp_log = []\n",
    "    max_ret = -10000\n",
    "    max_ret_hps = ()\n",
    "    max_ret_port = []\n",
    "    max_ret_hr = 0\n",
    "    max_ret_dd = 0\n",
    "    max_ret_dates = []\n",
    "    for hp in hps:\n",
    "        if version == '1' or version == '2':\n",
    "            ret1, _, portfolio_values1, hits1, dates1 = pairs_trade_strategy(merged_df, \n",
    "                                                                         lookback = hp[0], \n",
    "                                                                         entry = hp[1], \n",
    "                                                                         exit = hp[2], \n",
    "                                                                         stop_loss = hp[3], \n",
    "                                                                         end_date = oos_start_date)\n",
    "            ret2, _, portfolio_values2, hits2, dates2 = pairs_trade_strategy(merged_df, \n",
    "                                                                             cash = 250000 if len(portfolio_values1) == 0 else portfolio_values1[-1],\n",
    "                                                                             lookback = hp[0], \n",
    "                                                                             entry = hp[1], \n",
    "                                                                             exit = hp[2], \n",
    "                                                                             stop_loss = hp[3],\n",
    "                                                                             start_date = oos_end_date)\n",
    "            ret = (portfolio_values2[-1] - 250000) / 250000\n",
    "            portfolio_values = portfolio_values1 + portfolio_values2\n",
    "            hits = hits1 + hits2\n",
    "            dates = dates1 + dates2\n",
    "        else:\n",
    "            ret1, _, portfolio_values1, hits1, dates1 = pairs_trade_strategy(merged_df, \n",
    "                                                                         lookback = hp[0], \n",
    "                                                                         entry_cond1_val = hp[1], \n",
    "                                                                         entry_cond2_val = hp[1],\n",
    "                                                                         exit_cond1_val = hp[2],\n",
    "                                                                         exit_cond2_val = hp[2],\n",
    "                                                                         stop_loss_cond1 = hp[3],\n",
    "                                                                         stop_loss_cond2 = hp[3],\n",
    "                                                                         end_date = oos_start_date)\n",
    "            ret2, _, portfolio_values2, hits2, dates2 = pairs_trade_strategy(merged_df, \n",
    "                                                                             cash = 250000 if len(portfolio_values1) == 0 else portfolio_values1[-1],\n",
    "                                                                             lookback = hp[0], \n",
    "                                                                             entry_cond1_val = hp[1], \n",
    "                                                                             entry_cond2_val = hp[1],\n",
    "                                                                             exit_cond1_val = hp[2],\n",
    "                                                                             exit_cond2_val = hp[2],\n",
    "                                                                             stop_loss_cond1 = hp[3],\n",
    "                                                                             stop_loss_cond2 = hp[3],\n",
    "                                                                             start_date = oos_end_date)\n",
    "            ret = (portfolio_values2[-1] - 250000) / 250000\n",
    "            portfolio_values = portfolio_values1 + portfolio_values2\n",
    "            hits = hits1 + hits2\n",
    "            dates = dates1 + dates2\n",
    "        \n",
    "        ret = np.round(ret*100, 2)\n",
    "        hit_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        if hits:\n",
    "            hit_ratio = np.round(np.mean(hits)*100,2)\n",
    "            max_drawdown = np.round(calc_max_drawdown(portfolio_values)*100,2)\n",
    "        if ret > max_ret:\n",
    "            max_ret = ret\n",
    "            max_ret_hps = hp\n",
    "            max_ret_port = portfolio_values\n",
    "            max_ret_dates = dates\n",
    "            max_ret_hr = hit_ratio\n",
    "            max_ret_dd = max_drawdown\n",
    "        hp_log.append(f'{hp}: Return: {ret}%, Hit Ratio: {hit_ratio}%, Max Drawdown: {max_drawdown}%\\n')\n",
    "    logs = [f'(Lookback, Entry, Exit, Stop-loss)\\n',\n",
    "            f'Best HPs: {max_ret_hps}, Return: {max_ret}%, Hit Ratio: {max_ret_hr}%, Max Drawdown: {max_ret_dd}%\\n']\n",
    "    logs = logs + hp_log \n",
    "    if fine:\n",
    "        fname = f'eric_jh_data/{country}/{adr}/hp_log_fine_sfx{version}.txt' \n",
    "    else:\n",
    "        fname = f'eric_jh_data/{country}/{adr}/hp_log_sfx{version}.txt' \n",
    "    f = open(fname, 'w')\n",
    "    f.writelines(logs)\n",
    "    f.close()\n",
    "    if fine:\n",
    "        plot_returns(max_ret_dates, max_ret_port, country, adr, save = True, filename = f'is_sfx_fine_hp_pnl_plot_v{version}.png')\n",
    "    else:\n",
    "        plot_returns(max_ret_dates, max_ret_port, country, adr, save = True, filename = f'is_sfx_hp_pnl_plot_v{version}.png')\n",
    "    \n",
    "    best_hps = f'Country: {country}, ADR_Stock: {adr}, HPs: {max_ret_hps}, Return: {max_ret}%\\n'\n",
    "    if fine:\n",
    "        fname = f'results_fine_sfx{version}.txt'\n",
    "    else:\n",
    "        fname = f'results_sfx{version}.txt'\n",
    "    f = open(fname, 'a')\n",
    "    f.writelines(best_hps)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed:  1.2min remaining: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed:  2.1min remaining:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed:  2.2min remaining:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:  2.4min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed:  2.8min remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed:  3.6min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed:  4.0min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed:  4.1min remaining:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:  4.2min finished\n"
     ]
    }
   ],
   "source": [
    "v1 = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune)(pairs_trade_v1, '1', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed:  1.3min remaining: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed:  2.3min remaining:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed:  2.4min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:  2.5min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed:  2.6min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed:  3.6min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed:  4.1min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed:  4.2min remaining:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "v2 = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune)(pairs_trade_v2, '2', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed:  1.8min remaining: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed:  3.1min remaining: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed:  3.3min remaining:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:  3.4min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed:  3.6min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed:  4.8min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed:  5.4min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed:  5.7min remaining:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:  5.7min finished\n"
     ]
    }
   ],
   "source": [
    "v3a = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune)(pairs_trade_v3a, '3a', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed:  1.6min remaining: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed:  3.0min remaining: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed:  3.4min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:  3.6min remaining:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed:  3.6min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed:  4.8min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed:  5.4min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed:  5.8min remaining:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:  5.9min finished\n"
     ]
    }
   ],
   "source": [
    "v3b = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune)(pairs_trade_v3b, '3b', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate results for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(filename):\n",
    "    results = []\n",
    "    variants = ['1', '2', '3a', '3b']\n",
    "    for v in variants:\n",
    "        fname = f'{filename}{v}.txt'\n",
    "        with open(fname) as f:\n",
    "            results.append(f.read())\n",
    "\n",
    "    summ = []        \n",
    "    for (country, adr) in list_pairs:\n",
    "        summ.append(f'Country: {country}, ADR_Stock: {adr}\\n')\n",
    "        for v in range(4):\n",
    "            ind = results[v].find(f'Country: {country}, ADR_Stock: {adr}')\n",
    "            end = results[v][ind:].find('\\n')\n",
    "            res = results[v][ind + len(f'Country: {country}, ADR_Stock: {adr}, '):ind + end]\n",
    "            summ.append(f'Variant {variants[v]}: ' + res + '\\n')\n",
    "\n",
    "    fname = f'{filename}_all.txt'\n",
    "    f = open(fname, 'w')\n",
    "    f.writelines(summ)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate('results_sfx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort results for each pair based on return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = ['1', '2', '3a', '3b']\n",
    "\n",
    "def sort_res(string):\n",
    "    ind = string.find('Return: ')\n",
    "    end = string[ind:].find('%')\n",
    "    return float(string[ind + 8:ind + end])\n",
    "\n",
    "def sort_results(filename):\n",
    "    for (country, adr) in list_pairs:\n",
    "        for v in variants:\n",
    "            fname = f'eric_jh_data/{country}/{adr}/{filename}{v}.txt' \n",
    "            f = open(fname, 'r')\n",
    "            res = f.readlines()\n",
    "            f.close()\n",
    "            sorted_res = sorted(res[2:], key = sort_res, reverse = True)\n",
    "            res = res[:2] + sorted_res\n",
    "            f = open(fname, 'w')\n",
    "            f.writelines(res)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_results('hp_log_sfx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finer HP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "\"\"\"\n",
    "HP tune fine grid around coarse values\n",
    "\"\"\"\n",
    "\n",
    "def hp_tune_fine(pairs_trade_strategy, version, country, adr):\n",
    "    fname = f'eric_jh_data/{country}/{adr}/hp_log_sfx{version}.txt' \n",
    "    f = open(fname, 'r')\n",
    "    res = f.readlines()\n",
    "    f.close()\n",
    "    ind = res[1].find(', Return: ')\n",
    "    hps = res[1][:ind]\n",
    "    coarse_hps = tuple(map(float, hps[11:-1].split(', ')))\n",
    "    fine_window = tuple( map(add, [int(coarse_hps[0])]*5, [-10, 5, 0, 5, 10]) )\n",
    "    fine_entry = tuple( map(add, [coarse_hps[1]]*5, [-0.2, -0.1, 0, 0.1, 0.2]) )\n",
    "    fine_exit =  tuple( map(add, [coarse_hps[2]]*5, [-0.2, -0.1, 0, 0.1, 0.2]) )\n",
    "    fine_stop_loss =  tuple( map(add, [coarse_hps[3]]*5, [-0.2, -0.1, 0, 0.1, 0.2]) )\n",
    "    \n",
    "    hp_tune(pairs_trade_strategy, version, country, adr, fine_window, fine_entry, fine_exit, fine_stop_loss, fine = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed: 10.3min remaining: 86.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed: 14.5min remaining: 53.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed: 17.3min remaining: 36.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed: 19.0min remaining: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed: 20.9min remaining: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed: 24.8min remaining: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed: 26.2min remaining:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed: 27.1min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed: 29.2min finished\n"
     ]
    }
   ],
   "source": [
    "v1 = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune_fine)(pairs_trade_v1, '1', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed: 11.1min remaining: 92.9min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed: 15.3min remaining: 56.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed: 21.3min remaining: 44.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed: 23.0min remaining: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed: 24.6min remaining: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed: 27.2min remaining: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed: 31.3min remaining: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed: 33.5min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed: 36.8min finished\n"
     ]
    }
   ],
   "source": [
    "v2 = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune_fine)(pairs_trade_v2, '2', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed: 12.6min remaining: 105.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed: 21.0min remaining: 76.9min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed: 25.5min remaining: 53.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed: 27.0min remaining: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed: 32.5min remaining: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed: 34.7min remaining: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed: 39.2min remaining: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed: 41.3min remaining:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed: 46.1min finished\n"
     ]
    }
   ],
   "source": [
    "v3a = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune_fine)(pairs_trade_v3a, '3a', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  28 | elapsed: 15.0min remaining: 124.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  28 | elapsed: 20.9min remaining: 76.8min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  28 | elapsed: 26.1min remaining: 55.1min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed: 33.6min remaining: 44.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  28 | elapsed: 36.7min remaining: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  28 | elapsed: 38.9min remaining: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  28 | elapsed: 43.4min remaining: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  28 | elapsed: 45.0min remaining:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed: 51.2min finished\n"
     ]
    }
   ],
   "source": [
    "v3b = Parallel(n_jobs=-1, verbose = 10)(delayed(hp_tune_fine)(pairs_trade_v3b, '3b', country, adr) for (country, adr) in list_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate('results_fine_sfx')\n",
    "sort_results('hp_log_fine_sfx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate IS trade logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_trade_strategy = [pairs_trade_v1, pairs_trade_v2, pairs_trade_v3a, pairs_trade_v3b]\n",
    "\n",
    "for (country, adr) in list_pairs:\n",
    "    merged_df = data_processing(country, adr, fx_dict)\n",
    "    for i in range(4):\n",
    "        v = variants[i]\n",
    "        strat = pairs_trade_strategy[i]\n",
    "        fname = f'eric_jh_data/{country}/{adr}/hp_log_fine_sfx{v}.txt' \n",
    "        f = open(fname, 'r')\n",
    "        res = f.readlines()\n",
    "        f.close()\n",
    "        ind = res[1].find(', Return: ')\n",
    "        hps = res[1][:ind]\n",
    "        best_hps = tuple(map(float, hps[11:-1].split(', ')))\n",
    "        if i < 2:\n",
    "            ret1, tr1, pv1, hits1, dates1 = strat(merged_df, \n",
    "                                                 cash = 250000,\n",
    "                                                 lookback = int(best_hps[0]), \n",
    "                                                 entry = best_hps[1], \n",
    "                                                 exit = best_hps[2],\n",
    "                                                 stop_loss = best_hps[3],\n",
    "                                                 end_date = oos_start_date)\n",
    "            ret2, tr2, pv2, hits2, dates2 = strat(merged_df, \n",
    "                                                 cash = 250000 if len(pv1) == 0 else pv1[-1],\n",
    "                                                 lookback = int(best_hps[0]), \n",
    "                                                 entry = best_hps[1], \n",
    "                                                 exit = best_hps[2],\n",
    "                                                 stop_loss = best_hps[3],\n",
    "                                                 start_date = oos_end_date)\n",
    "            ret = (pv2[-1] - 250000) / 250000\n",
    "            tr = tr1 + tr2\n",
    "            pv = pv1 + pv2\n",
    "            hits = hits1 + hits2\n",
    "            dates = dates1 + dates2\n",
    "            \n",
    "        else:\n",
    "            ret1, tr1, pv1, hits1, dates1 = strat(merged_df, \n",
    "                                                 cash = 250000,\n",
    "                                                 lookback = int(best_hps[0]), \n",
    "                                                 entry_cond1_val = best_hps[1], \n",
    "                                                 entry_cond2_val = best_hps[1],\n",
    "                                                 exit_cond1_val = best_hps[2],\n",
    "                                                 exit_cond2_val = best_hps[2],\n",
    "                                                 stop_loss_cond1 = best_hps[3],\n",
    "                                                 stop_loss_cond2 = best_hps[3],\n",
    "                                                 end_date = oos_start_date)\n",
    "            ret2, tr2, pv2, hits2, dates2 = strat(merged_df, \n",
    "                                                 cash = 250000 if len(pv1) == 0 else pv1[-1],\n",
    "                                                 lookback = int(best_hps[0]), \n",
    "                                                 entry_cond1_val = best_hps[1], \n",
    "                                                 entry_cond2_val = best_hps[1],\n",
    "                                                 exit_cond1_val = best_hps[2],\n",
    "                                                 exit_cond2_val = best_hps[2],\n",
    "                                                 stop_loss_cond1 = best_hps[3],\n",
    "                                                 stop_loss_cond2 = best_hps[3],\n",
    "                                                 start_date = oos_end_date)\n",
    "            ret = (pv2[-1] - 250000) / 250000\n",
    "            tr = tr1 + tr2\n",
    "            pv = pv1 + pv2\n",
    "            hits = hits1 + hits2\n",
    "            dates = dates1 + dates2\n",
    "            \n",
    "        ret = np.round(ret*100, 2)\n",
    "        hit_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        if hits:\n",
    "            hit_ratio = np.round(np.mean(hits)*100,2)\n",
    "            max_drawdown = np.round(calc_max_drawdown(pv)*100,2)\n",
    "            \n",
    "        plot_returns(dates, pv, country, adr, save = True, filename = f'is_sfx_hp_pnl_plot_v{v}.png')\n",
    "        \n",
    "        result = f'Country: {country}\\nADR: {adr}\\nReturn: {ret}%\\nHit Ratio: {hit_ratio}%\\nMax Drawdown: {max_drawdown}%\\n'\n",
    "        result += f'\\nTrades\\n\\n'\n",
    "        \n",
    "        fname = f'eric_jh_data/{country}/{adr}/is_log_sfx{v}.txt' \n",
    "        f = open(fname, 'w')\n",
    "        f.writelines(result)\n",
    "        f.writelines(tr)\n",
    "        f.writelines(f'\\nHyperparameters\\n\\n{best_hps}')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Out-of-sample Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia ATHE_ATH Variant1, Return: 0.08%, Hit Ratio: 78.95%, Max Drawdown: 0.01%\n",
      "Australia ATHE_ATH Variant2, Return: 0.05%, Hit Ratio: 84.62%, Max Drawdown: 0.02%\n",
      "Australia ATHE_ATH Variant3a, Return: 0.04%, Hit Ratio: 82.35%, Max Drawdown: 0.03%\n",
      "Australia ATHE_ATH Variant3b, Return: 0.02%, Hit Ratio: 76.92%, Max Drawdown: 0.03%\n",
      "Australia GENE_GTG Variant1, Return: 0.17%, Hit Ratio: 86.67%, Max Drawdown: 0.02%\n",
      "Australia GENE_GTG Variant2, Return: 0.11%, Hit Ratio: 81.25%, Max Drawdown: 0.02%\n",
      "Australia GENE_GTG Variant3a, Return: 0.22%, Hit Ratio: 88.89%, Max Drawdown: 0.06%\n",
      "Australia GENE_GTG Variant3b, Return: 0.15%, Hit Ratio: 76.92%, Max Drawdown: 0.04%\n",
      "Australia IMMP_IMM Variant1, Return: 0.01%, Hit Ratio: 100.0%, Max Drawdown: 0.01%\n",
      "Australia IMMP_IMM Variant2, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia IMMP_IMM Variant3a, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia IMMP_IMM Variant3b, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia IMRN_IMC Variant1, Return: 0.01%, Hit Ratio: 66.67%, Max Drawdown: 0.01%\n",
      "Australia IMRN_IMC Variant2, Return: 0.04%, Hit Ratio: 73.33%, Max Drawdown: 0.02%\n",
      "Australia IMRN_IMC Variant3a, Return: 0.08%, Hit Ratio: 70.0%, Max Drawdown: 0.01%\n",
      "Australia IMRN_IMC Variant3b, Return: 0.08%, Hit Ratio: 73.91%, Max Drawdown: 0.02%\n",
      "Australia JHX_JHX Variant1, Return: -0.01%, Hit Ratio: 28.57%, Max Drawdown: 0.02%\n",
      "Australia JHX_JHX Variant2, Return: 0.0%, Hit Ratio: 33.33%, Max Drawdown: 0.01%\n",
      "Australia JHX_JHX Variant3a, Return: -0.01%, Hit Ratio: 34.21%, Max Drawdown: 0.02%\n",
      "Australia JHX_JHX Variant3b, Return: -0.03%, Hit Ratio: 40.0%, Max Drawdown: 0.03%\n",
      "Australia KZIA_KZA Variant1, Return: 0.04%, Hit Ratio: 84.21%, Max Drawdown: 0.01%\n",
      "Australia KZIA_KZA Variant2, Return: 0.01%, Hit Ratio: 100.0%, Max Drawdown: 0.0%\n",
      "Australia KZIA_KZA Variant3a, Return: 0.07%, Hit Ratio: 83.33%, Max Drawdown: 0.02%\n",
      "Australia KZIA_KZA Variant3b, Return: 0.05%, Hit Ratio: 86.67%, Max Drawdown: 0.02%\n",
      "Australia MESO_MSB Variant1, Return: 0.03%, Hit Ratio: 75.0%, Max Drawdown: 0.02%\n",
      "Australia MESO_MSB Variant2, Return: 0.15%, Hit Ratio: 85.71%, Max Drawdown: 0.02%\n",
      "Australia MESO_MSB Variant3a, Return: 0.15%, Hit Ratio: 79.41%, Max Drawdown: 0.05%\n",
      "Australia MESO_MSB Variant3b, Return: 0.14%, Hit Ratio: 82.61%, Max Drawdown: 0.02%\n",
      "Australia PLL_PLL Variant1, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia PLL_PLL Variant2, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia PLL_PLL Variant3a, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia PLL_PLL Variant3b, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Australia WBK_WBC Variant1, Return: 0.1%, Hit Ratio: 66.67%, Max Drawdown: 0.13%\n",
      "Australia WBK_WBC Variant2, Return: -0.21%, Hit Ratio: 58.82%, Max Drawdown: 0.43%\n",
      "Australia WBK_WBC Variant3a, Return: -0.13%, Hit Ratio: 37.5%, Max Drawdown: 0.35%\n",
      "Australia WBK_WBC Variant3b, Return: -0.31%, Hit Ratio: 38.89%, Max Drawdown: 0.45%\n",
      "China ACH_2600 Variant1, Return: 0.21%, Hit Ratio: 60.0%, Max Drawdown: 0.1%\n",
      "China ACH_2600 Variant2, Return: 0.09%, Hit Ratio: 53.33%, Max Drawdown: 0.05%\n",
      "China ACH_2600 Variant3a, Return: 0.05%, Hit Ratio: 54.55%, Max Drawdown: 0.17%\n",
      "China ACH_2600 Variant3b, Return: 0.08%, Hit Ratio: 51.43%, Max Drawdown: 0.1%\n",
      "China BGNE_6160 Variant1, Return: 0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "China BGNE_6160 Variant2, Return: 0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "China BGNE_6160 Variant3a, Return: 0%, Hit Ratio: 0%, Max Drawdown: 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binyu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Attempting to set identical left == right == 0 results in singular transformations; automatically expanding.\n",
      "  \n",
      "C:\\Users\\binyu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Attempting to set identical left == right == 0 results in singular transformations; automatically expanding.\n",
      "  \n",
      "C:\\Users\\binyu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Attempting to set identical left == right == 0 results in singular transformations; automatically expanding.\n",
      "  \n",
      "C:\\Users\\binyu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Attempting to set identical left == right == 0 results in singular transformations; automatically expanding.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China BGNE_6160 Variant3b, Return: 0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "China CEA_670 Variant1, Return: 0.15%, Hit Ratio: 66.67%, Max Drawdown: 0.1%\n",
      "China CEA_670 Variant2, Return: 0.14%, Hit Ratio: 65.52%, Max Drawdown: 0.04%\n",
      "China CEA_670 Variant3a, Return: 0.02%, Hit Ratio: 57.14%, Max Drawdown: 0.1%\n",
      "China CEA_670 Variant3b, Return: -0.12%, Hit Ratio: 57.14%, Max Drawdown: 0.19%\n",
      "China HNP_902 Variant1, Return: 0.01%, Hit Ratio: 37.5%, Max Drawdown: 0.15%\n",
      "China HNP_902 Variant2, Return: -0.0%, Hit Ratio: 50.0%, Max Drawdown: 0.09%\n",
      "China HNP_902 Variant3a, Return: 0.01%, Hit Ratio: 42.11%, Max Drawdown: 0.14%\n",
      "China HNP_902 Variant3b, Return: -0.06%, Hit Ratio: 43.75%, Max Drawdown: 0.13%\n",
      "China LFC_2628 Variant1, Return: 0.01%, Hit Ratio: 50.0%, Max Drawdown: 0.13%\n",
      "China LFC_2628 Variant2, Return: -0.31%, Hit Ratio: 41.18%, Max Drawdown: 0.5%\n",
      "China LFC_2628 Variant3a, Return: -0.38%, Hit Ratio: 41.18%, Max Drawdown: 0.5%\n",
      "China LFC_2628 Variant3b, Return: -0.38%, Hit Ratio: 27.27%, Max Drawdown: 0.7%\n",
      "China PTR_857 Variant1, Return: -0.42%, Hit Ratio: 0.0%, Max Drawdown: 0.42%\n",
      "China PTR_857 Variant2, Return: 0.06%, Hit Ratio: 100.0%, Max Drawdown: 0.02%\n",
      "China PTR_857 Variant3a, Return: 0.04%, Hit Ratio: 50.0%, Max Drawdown: 0.06%\n",
      "China PTR_857 Variant3b, Return: 0.07%, Hit Ratio: 100.0%, Max Drawdown: 0.03%\n",
      "China SHI_338 Variant1, Return: 0.28%, Hit Ratio: 66.67%, Max Drawdown: 0.05%\n",
      "China SHI_338 Variant2, Return: -0.21%, Hit Ratio: 50.0%, Max Drawdown: 0.22%\n",
      "China SHI_338 Variant3a, Return: -0.03%, Hit Ratio: 41.38%, Max Drawdown: 0.19%\n",
      "China SHI_338 Variant3b, Return: 0.14%, Hit Ratio: 55.26%, Max Drawdown: 0.13%\n",
      "China SNP_386 Variant1, Return: 0.03%, Hit Ratio: 71.43%, Max Drawdown: 0.36%\n",
      "China SNP_386 Variant2, Return: -0.74%, Hit Ratio: 56.25%, Max Drawdown: 1.14%\n",
      "China SNP_386 Variant3a, Return: -1.97%, Hit Ratio: 48.78%, Max Drawdown: 2.82%\n",
      "China SNP_386 Variant3b, Return: -1.98%, Hit Ratio: 47.5%, Max Drawdown: 2.83%\n",
      "China ZNH_1055 Variant1, Return: -0.29%, Hit Ratio: 31.82%, Max Drawdown: 0.44%\n",
      "China ZNH_1055 Variant2, Return: 0.12%, Hit Ratio: 52.94%, Max Drawdown: 0.21%\n",
      "China ZNH_1055 Variant3a, Return: -0.1%, Hit Ratio: 44.44%, Max Drawdown: 0.11%\n",
      "China ZNH_1055 Variant3b, Return: -0.1%, Hit Ratio: 57.14%, Max Drawdown: 0.12%\n",
      "Japan CAJ_7751 Variant1, Return: -0.27%, Hit Ratio: 20.0%, Max Drawdown: 0.31%\n",
      "Japan CAJ_7751 Variant2, Return: -0.59%, Hit Ratio: 41.67%, Max Drawdown: 0.72%\n",
      "Japan CAJ_7751 Variant3a, Return: -0.68%, Hit Ratio: 50.0%, Max Drawdown: 0.77%\n",
      "Japan CAJ_7751 Variant3b, Return: -0.29%, Hit Ratio: 50.0%, Max Drawdown: 0.44%\n",
      "Japan HMC_7267 Variant1, Return: -0.06%, Hit Ratio: 0.0%, Max Drawdown: 0.27%\n",
      "Japan HMC_7267 Variant2, Return: 0.01%, Hit Ratio: 20.0%, Max Drawdown: 0.86%\n",
      "Japan HMC_7267 Variant3a, Return: -0.82%, Hit Ratio: 33.33%, Max Drawdown: 1.51%\n",
      "Japan HMC_7267 Variant3b, Return: -0.21%, Hit Ratio: 20.0%, Max Drawdown: 0.65%\n",
      "Japan IX_8591 Variant1, Return: -0.26%, Hit Ratio: 0.0%, Max Drawdown: 0.26%\n",
      "Japan IX_8591 Variant2, Return: -0.13%, Hit Ratio: 27.27%, Max Drawdown: 0.24%\n",
      "Japan IX_8591 Variant3a, Return: -0.26%, Hit Ratio: 28.57%, Max Drawdown: 0.29%\n",
      "Japan IX_8591 Variant3b, Return: -0.15%, Hit Ratio: 53.33%, Max Drawdown: 0.25%\n",
      "Japan MFG_8411 Variant1, Return: 0.0%, Hit Ratio: 50.0%, Max Drawdown: 0.01%\n",
      "Japan MFG_8411 Variant2, Return: -0.05%, Hit Ratio: 53.85%, Max Drawdown: 0.06%\n",
      "Japan MFG_8411 Variant3a, Return: -0.07%, Hit Ratio: 40.0%, Max Drawdown: 0.1%\n",
      "Japan MFG_8411 Variant3b, Return: -0.06%, Hit Ratio: 53.85%, Max Drawdown: 0.07%\n",
      "Japan MUFG_8306 Variant1, Return: -0.33%, Hit Ratio: 33.33%, Max Drawdown: 0.59%\n",
      "Japan MUFG_8306 Variant2, Return: 0.16%, Hit Ratio: 60.87%, Max Drawdown: 0.67%\n",
      "Japan MUFG_8306 Variant3a, Return: -1.3%, Hit Ratio: 45.0%, Max Drawdown: 1.77%\n",
      "Japan MUFG_8306 Variant3b, Return: -1.01%, Hit Ratio: 50.0%, Max Drawdown: 1.37%\n",
      "Japan NMR_8604 Variant1, Return: 0.0%, Hit Ratio: 0%, Max Drawdown: 0%\n",
      "Japan NMR_8604 Variant2, Return: 0.03%, Hit Ratio: 52.63%, Max Drawdown: 0.08%\n",
      "Japan NMR_8604 Variant3a, Return: -0.12%, Hit Ratio: 40.62%, Max Drawdown: 0.16%\n",
      "Japan NMR_8604 Variant3b, Return: -0.02%, Hit Ratio: 0.0%, Max Drawdown: 0.02%\n",
      "Japan SMFG_8316 Variant1, Return: -0.22%, Hit Ratio: 0.0%, Max Drawdown: 0.22%\n",
      "Japan SMFG_8316 Variant2, Return: 0.07%, Hit Ratio: 50.0%, Max Drawdown: 0.58%\n",
      "Japan SMFG_8316 Variant3a, Return: 0.39%, Hit Ratio: 63.16%, Max Drawdown: 0.31%\n",
      "Japan SMFG_8316 Variant3b, Return: -0.29%, Hit Ratio: 50.0%, Max Drawdown: 0.87%\n",
      "Japan SONY_6758 Variant1, Return: -1.56%, Hit Ratio: 66.67%, Max Drawdown: 3.38%\n",
      "Japan SONY_6758 Variant2, Return: 5.57%, Hit Ratio: 66.67%, Max Drawdown: 1.08%\n",
      "Japan SONY_6758 Variant3a, Return: -1.02%, Hit Ratio: 60.0%, Max Drawdown: 3.75%\n",
      "Japan SONY_6758 Variant3b, Return: -3.81%, Hit Ratio: 50.0%, Max Drawdown: 5.06%\n",
      "Japan TAK_4502 Variant1, Return: -0.01%, Hit Ratio: 60.0%, Max Drawdown: 0.04%\n",
      "Japan TAK_4502 Variant2, Return: -0.14%, Hit Ratio: 29.03%, Max Drawdown: 0.15%\n",
      "Japan TAK_4502 Variant3a, Return: -0.14%, Hit Ratio: 33.33%, Max Drawdown: 0.15%\n",
      "Japan TAK_4502 Variant3b, Return: -0.11%, Hit Ratio: 30.77%, Max Drawdown: 0.12%\n",
      "Japan TM_7203 Variant1, Return: -0.26%, Hit Ratio: 33.33%, Max Drawdown: 0.33%\n",
      "Japan TM_7203 Variant2, Return: 0.18%, Hit Ratio: 66.67%, Max Drawdown: 0.61%\n",
      "Japan TM_7203 Variant3a, Return: -0.4%, Hit Ratio: 60.0%, Max Drawdown: 0.7%\n",
      "Japan TM_7203 Variant3b, Return: -0.97%, Hit Ratio: 52.94%, Max Drawdown: 1.29%\n"
     ]
    }
   ],
   "source": [
    "pairs_trade_strategy = [pairs_trade_v1, pairs_trade_v2, pairs_trade_v3a, pairs_trade_v3b]\n",
    "\n",
    "for (country, adr) in list_pairs:\n",
    "    merged_df = data_processing(country, adr, fx_dict)\n",
    "    for i in range(4):\n",
    "        v = variants[i]\n",
    "        strat = pairs_trade_strategy[i]\n",
    "        fname = f'eric_jh_data/{country}/{adr}/hp_log_fine_sfx{v}.txt' \n",
    "        f = open(fname, 'r')\n",
    "        res = f.readlines()\n",
    "        f.close()\n",
    "        ind = res[1].find(', Return: ')\n",
    "        hps = res[1][:ind]\n",
    "        best_hps = tuple(map(float, hps[11:-1].split(', ')))\n",
    "        if i < 2:\n",
    "            ret, tr, pv, hits, dates = strat(merged_df, \n",
    "                             cash = 250000,\n",
    "                             lookback = int(best_hps[0]), \n",
    "                             entry = best_hps[1], \n",
    "                             exit = best_hps[2],\n",
    "                             stop_loss = best_hps[3],\n",
    "                             start_date = oos_start_date, \n",
    "                             end_date = oos_end_date)\n",
    "        else:\n",
    "            ret, tr, pv, hits, dates = strat(merged_df, \n",
    "                             cash = 250000,\n",
    "                             lookback = int(best_hps[0]), \n",
    "                             entry_cond1_val = best_hps[1], \n",
    "                             entry_cond2_val = best_hps[1],\n",
    "                             exit_cond1_val = best_hps[2],\n",
    "                             exit_cond2_val = best_hps[2],\n",
    "                             stop_loss_cond1 = best_hps[3],\n",
    "                             stop_loss_cond2 = best_hps[3],\n",
    "                             start_date = oos_start_date, \n",
    "                             end_date = oos_end_date)\n",
    "            \n",
    "        ret = np.round(ret*100, 2)\n",
    "        hit_ratio = 0\n",
    "        max_drawdown = 0\n",
    "        if hits:\n",
    "            hit_ratio = np.round(np.mean(hits)*100,2)\n",
    "            max_drawdown = np.round(calc_max_drawdown(pv)*100,2)\n",
    "            \n",
    "        plot_returns(dates, pv, country, adr, save = True, filename = f'oos_sfx_hp_pnl_plot_v{v}.png')\n",
    "        \n",
    "        result = f'Country: {country}\\nADR: {adr}\\nReturn: {ret}%\\nHit Ratio: {hit_ratio}%\\nMax Drawdown: {max_drawdown}%\\n'\n",
    "        result += f'\\nTrades\\n\\n'\n",
    "        \n",
    "        fname = f'eric_jh_data/{country}/{adr}/oos_log_sfx{v}.txt' \n",
    "        f = open(fname, 'w')\n",
    "        f.writelines(result)\n",
    "        f.writelines(tr)\n",
    "        f.writelines(f'\\nHyperparameters\\n\\n{best_hps}')\n",
    "        f.close()\n",
    "        \n",
    "        fname = 'oos_sfx_cum_results.txt'\n",
    "        f = open(fname, 'a')\n",
    "        f.writelines(f'{country} {adr} Variant{v}, Return: {ret}%, Hit Ratio: {hit_ratio}%, Max Drawdown: {max_drawdown}%\\n')\n",
    "        f.close()\n",
    "        print(f'{country} {adr} Variant{v}, Return: {ret}%, Hit Ratio: {hit_ratio}%, Max Drawdown: {max_drawdown}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store best variant + HPs to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'results_sfx_all.txt'\n",
    "f = open(fname, 'r')\n",
    "res = f.readlines()\n",
    "f.close()\n",
    "\n",
    "column_names = ['country', 'adr_stock', 'lookback', 'entry', 'exit', 'stoploss', 'variant']\n",
    "summ_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "for (country, adr) in list_pairs:\n",
    "    best_ret = -1\n",
    "    variant = \"\"\n",
    "    lookback = 100\n",
    "    entry = 1\n",
    "    exit = 0\n",
    "    stoploss = 3\n",
    "    ind = res.index(f'Country: {country}, ADR_Stock: {adr}\\n')\n",
    "    for v in range(1,5):\n",
    "        ret_ind = res[ind + v].find('Return: ')\n",
    "        ret = float(res[ind + v][ret_ind + 8 : -2])\n",
    "        if ret > best_ret:\n",
    "            variant = variants[v - 1]\n",
    "            hp_ind_start = res[ind + v].find('HPs: ')\n",
    "            hp_ind_end = res[ind + v].find(', Return: ')\n",
    "            hps = res[ind + v][hp_ind_start + 5:hp_ind_end]\n",
    "            best_hps = hps[1:-1].split(', ')\n",
    "            lookback = int(best_hps[0])\n",
    "            entry = float(best_hps[1])\n",
    "            exit = float(best_hps[2])\n",
    "            stoploss = float(best_hps[3])\n",
    "            best_ret = ret\n",
    "    temp_df = pd.DataFrame([[country, adr, lookback, entry, exit, stoploss, variant]], columns = column_names)\n",
    "    summ_df = summ_df.append(temp_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>adr_stock</th>\n",
       "      <th>lookback</th>\n",
       "      <th>entry</th>\n",
       "      <th>exit</th>\n",
       "      <th>stoploss</th>\n",
       "      <th>variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>ATHE_ATH</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>GENE_GTG</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>IMMP_IMM</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>IMRN_IMC</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>JHX_JHX</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia</td>\n",
       "      <td>KZIA_KZA</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Australia</td>\n",
       "      <td>MESO_MSB</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Australia</td>\n",
       "      <td>PLL_PLL</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Australia</td>\n",
       "      <td>WBK_WBC</td>\n",
       "      <td>100</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>China</td>\n",
       "      <td>ACH_2600</td>\n",
       "      <td>30</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China</td>\n",
       "      <td>BGNE_6160</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>China</td>\n",
       "      <td>CEA_670</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>China</td>\n",
       "      <td>HNP_902</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>China</td>\n",
       "      <td>LFC_2628</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>China</td>\n",
       "      <td>PTR_857</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>China</td>\n",
       "      <td>SHI_338</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>China</td>\n",
       "      <td>SNP_386</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>China</td>\n",
       "      <td>ZNH_1055</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Japan</td>\n",
       "      <td>CAJ_7751</td>\n",
       "      <td>30</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Japan</td>\n",
       "      <td>HMC_7267</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Japan</td>\n",
       "      <td>IX_8591</td>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Japan</td>\n",
       "      <td>MFG_8411</td>\n",
       "      <td>60</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Japan</td>\n",
       "      <td>MUFG_8306</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Japan</td>\n",
       "      <td>NMR_8604</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Japan</td>\n",
       "      <td>SMFG_8316</td>\n",
       "      <td>30</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Japan</td>\n",
       "      <td>SONY_6758</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Japan</td>\n",
       "      <td>TAK_4502</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Japan</td>\n",
       "      <td>TM_7203</td>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country  adr_stock lookback  entry  exit  stoploss variant\n",
       "0   Australia   ATHE_ATH      100    1.0  -0.5       2.5      3a\n",
       "1   Australia   GENE_GTG       30    1.0  -0.5       2.5      3a\n",
       "2   Australia   IMMP_IMM       60    1.0  -0.5       3.0       1\n",
       "3   Australia   IMRN_IMC       30    1.0   0.0       2.5      3b\n",
       "4   Australia    JHX_JHX      100    1.0   0.5       3.5      3a\n",
       "5   Australia   KZIA_KZA      100    1.0  -0.5       3.5      3b\n",
       "6   Australia   MESO_MSB       30    1.0  -0.5       3.5      3b\n",
       "7   Australia    PLL_PLL       60    2.0   0.5       3.0      3a\n",
       "8   Australia    WBK_WBC      100    1.5   0.5       3.5      3b\n",
       "9       China   ACH_2600       30    1.5   0.5       2.5       2\n",
       "10      China  BGNE_6160       60    1.0   0.0       3.5       1\n",
       "11      China    CEA_670       60    1.0   0.0       3.5       1\n",
       "12      China    HNP_902      100    1.0  -0.5       3.5      3a\n",
       "13      China   LFC_2628      100    1.0  -0.5       2.5       2\n",
       "14      China    PTR_857       60    2.0   0.0       2.5       1\n",
       "15      China    SHI_338      100    1.0  -0.5       2.5       1\n",
       "16      China    SNP_386       30    1.0   0.0       2.5      3b\n",
       "17      China   ZNH_1055      100    1.0  -0.5       3.5       1\n",
       "18      Japan   CAJ_7751       30    1.5  -0.5       2.5      3b\n",
       "19      Japan   HMC_7267       60    2.0   0.5       3.0       1\n",
       "20      Japan    IX_8591       30    2.0   0.5       3.5      3a\n",
       "21      Japan   MFG_8411       60    1.5  -0.5       2.5      3a\n",
       "22      Japan  MUFG_8306       30    1.0  -0.5       3.5       2\n",
       "23      Japan   NMR_8604       30    1.0  -0.5       3.5       2\n",
       "24      Japan  SMFG_8316       30    1.5  -0.5       3.0       2\n",
       "25      Japan  SONY_6758      100    1.0  -0.5       3.5       2\n",
       "26      Japan   TAK_4502      100    1.0   0.5       3.0      3b\n",
       "27      Japan    TM_7203       30    2.0   0.5       2.5      3a"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_df.to_csv('hps_sfx.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
